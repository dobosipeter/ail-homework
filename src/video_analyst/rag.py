""" This module is responsible for implementing the RAG pipeline element of the solution. """

import logging
from pathlib import Path
from typing import Any

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from video_analyst.segmentation import VideoStructure

VECTOR_STORE_DIR = Path("data/faiss_index")
VECTOR_STORE_DIR.mkdir(parents=True, exist_ok=True)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def _reconstruct_chapter_text(
    chapter_start: float,
    chapter_end: float,
    transcript_segments: list[dict[str, Any]]
) -> str:
    """
    Reconstructs the raw text for a specific time window from the transcript segments.

    Args:
        chapter_start (float): Start time in seconds.
        chapter_end (float): End time in seconds.
        transcript_segments (List[Dict]): Full list of transcript segments containing word-level timestamps.

    Returns:
        str: The combined raw text spoken during the specified time range.
    """
    chapter_words = []

    for segment in transcript_segments:
        for word_obj in segment["words"]:
            w_start = word_obj["start"]
            if chapter_start <= w_start < chapter_end:
                chapter_words.append(word_obj["word"])

    return " ".join(chapter_words)

def build_vector_store(
    video_path: str,
    chapters: VideoStructure,
    transcript_data: list[dict[str, Any]],
    collection_name: str
) -> FAISS:
    """
    Adds video chapters to a named FAISS index (Collection).

    If the collection exists, it loads it and appends the new video's documents.
    It performs a deduplication check based on the video filename to prevent 
    adding the same content twice.

    Args:
        video_path (str): Path to the video file (used for deduplication ID).
        chapters (VideoStructure): The structured chapters generated by the segmentation phase.
        transcript_data (list[dict]): Raw transcript data for text reconstruction.
        collection_name (str): The name of the collection (folder) to store this index in.

    Returns:
        FAISS: The updated vector store instance.
    """
    video_stem = Path(video_path).stem
    index_path = VECTOR_STORE_DIR / collection_name

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_store = None

    if (index_path / "index.faiss").exists():
        logger.info("Loading existing collection: %s", collection_name)
        try:
            vector_store = FAISS.load_local(str(index_path), embeddings, allow_dangerous_deserialization=True)

            # Deduplication: Check if this video is already in the index
            existing_sources = {
                doc.metadata.get("source_video")
                for doc in vector_store.docstore._dict.values()
            }

            if video_stem in existing_sources:
                logger.info("Video '%s' already in collection '%s'. Skipping.", video_stem, collection_name)
                return vector_store

        except Exception as e:
            logger.error("Failed to load existing index, creating new one. Error: %s", e)
            vector_store = None

    logger.info("Processing chapters for %s...", video_stem)
    documents = []

    for chapter in chapters.chapters:
        # Reconstruct "Truth" (Raw Text) for the LLM to read
        raw_text = _reconstruct_chapter_text(chapter.start_time, chapter.end_time, transcript_data)

        # Construct "Search Query" (Semantic Summary) for the Index
        search_content = f"Title: {chapter.title}\nSummary: {chapter.summary}\nKeywords: {', '.join(chapter.topic_keywords)}"

        doc = Document(
            page_content=search_content,
            metadata={
                "source_video": video_stem,
                "title": chapter.title,
                "start": chapter.start_time,
                "end": chapter.end_time,
                "raw_text": raw_text 
            }
        )
        documents.append(doc)

    if vector_store is None:
        logger.info("Creating NEW collection: %s", collection_name)
        vector_store = FAISS.from_documents(documents, embeddings)
    else:
        logger.info("Appending %d docs to collection: %s", len(documents), collection_name)
        vector_store.add_documents(documents)

    vector_store.save_local(str(index_path))
    logger.info("Collection saved to %s", index_path)

    return vector_store


def query_knowledge_base(query: str, collection_name: str) -> dict[str, Any]:
    """
    Queries a specific collection using RAG (Retrieval-Augmented Generation).

    Retrieves the most relevant chapter summaries, fetches their corresponding 
    raw text, and prompts an LLM to answer the user's question based strictly 
    on that context.

    Args:
        query (str): The user's question.
        collection_name (str): The name of the knowledge base collection to search.

    Returns:
        dict[str, Any]: A dictionary containing:
            - 'answer': The LLM generated response.
            - 'sources': A list of strings citing the chapters used.
            - 'context_used': The full raw text passed to the LLM (for debugging).

    Raises:
        FileNotFoundError: If the specified collection does not exist.
    """
    index_path = VECTOR_STORE_DIR / collection_name

    if not (index_path / "index.faiss").exists():
        raise FileNotFoundError(f"Collection '{collection_name}' not found.")

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_store = FAISS.load_local(str(index_path), embeddings, allow_dangerous_deserialization=True)

    # 1. Retrieve Context
    docs = vector_store.similarity_search(query, k=4)

    context_parts = []
    sources = []

    for doc in docs:
        meta = doc.metadata
        time_str = f"{int(meta['start']//60)}:{int(meta['start']%60):02d}"
        source_label = f"{meta['source_video']} - {meta['title']}"

        # We explicitly label the context so the LLM can cite it
        context_parts.append(f"--- Source: {source_label} (Starts at {time_str}) ---\n{meta['raw_text']}")
        sources.append(f"{source_label} ({time_str})")

    context_str = "\n\n".join(context_parts)

    # 2. Generate Answer
    llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful teaching assistant. Answer the user's question based ONLY on the provided context.\n"
                   "If the answer is not in the context, say you don't know.\n"
                   "Cite the video name and chapter titles when appropriate."),
        ("user", "Context:\n{context}\n\nQuestion: {question}")
    ])

    chain = prompt | llm | StrOutputParser()
    answer = chain.invoke({"context": context_str, "question": query})

    return {
        "answer": answer,
        "sources": sources,
        "context_used": context_str
    }


def list_collections() -> list[str]:
    """
    Lists the names of all available FAISS collections on disk.

    Returns:
        List[str]: A list of collection names (folder names) found in the vector store directory.
    """
    if not VECTOR_STORE_DIR.exists():
        return []

    return [
        d.name for d in VECTOR_STORE_DIR.iterdir()
        if d.is_dir() and (d / "index.faiss").exists()
    ]
